{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Breast_cancer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukD88JApqXDv"
      },
      "source": [
        "# **Import required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoqNUuNgmVc-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1p-cCzUqfr2"
      },
      "source": [
        "# **Reads .csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q13sx3z8mhLj",
        "outputId": "496623fe-d984-4748-f3e5-0c4f635ea6f1"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/Breast Cancer.csv\")\n",
        "dataset.info()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 569 entries, 0 to 568\n",
            "Data columns (total 33 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   id                       569 non-null    int64  \n",
            " 1   diagnosis                569 non-null    object \n",
            " 2   radius_mean              569 non-null    float64\n",
            " 3   texture_mean             569 non-null    float64\n",
            " 4   perimeter_mean           569 non-null    float64\n",
            " 5   area_mean                569 non-null    float64\n",
            " 6   smoothness_mean          569 non-null    float64\n",
            " 7   compactness_mean         569 non-null    float64\n",
            " 8   concavity_mean           569 non-null    float64\n",
            " 9   concave points_mean      569 non-null    float64\n",
            " 10  symmetry_mean            569 non-null    float64\n",
            " 11  fractal_dimension_mean   569 non-null    float64\n",
            " 12  radius_se                569 non-null    float64\n",
            " 13  texture_se               569 non-null    float64\n",
            " 14  perimeter_se             569 non-null    float64\n",
            " 15  area_se                  569 non-null    float64\n",
            " 16  smoothness_se            569 non-null    float64\n",
            " 17  compactness_se           569 non-null    float64\n",
            " 18  concavity_se             569 non-null    float64\n",
            " 19  concave points_se        569 non-null    float64\n",
            " 20  symmetry_se              569 non-null    float64\n",
            " 21  fractal_dimension_se     569 non-null    float64\n",
            " 22  radius_worst             569 non-null    float64\n",
            " 23  texture_worst            569 non-null    float64\n",
            " 24  perimeter_worst          569 non-null    float64\n",
            " 25  area_worst               569 non-null    float64\n",
            " 26  smoothness_worst         569 non-null    float64\n",
            " 27  compactness_worst        569 non-null    float64\n",
            " 28  concavity_worst          569 non-null    float64\n",
            " 29  concave points_worst     569 non-null    float64\n",
            " 30  symmetry_worst           569 non-null    float64\n",
            " 31  fractal_dimension_worst  569 non-null    float64\n",
            " 32  Unnamed: 32              0 non-null      float64\n",
            "dtypes: float64(31), int64(1), object(1)\n",
            "memory usage: 146.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIWhh1OmqkCT"
      },
      "source": [
        "**1.** Splits the data in to x and y\n",
        "\n",
        "**2.** Transforms y data by labelEncoder as it is a categorical data\n",
        "\n",
        "**3.** Splits data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn60UZqAmsvI"
      },
      "source": [
        "X = dataset.iloc[:,2:32].values\n",
        "y = dataset.iloc[:,1].values\n",
        "\n",
        "# Trnasform y data by labelEncoder as it is a categorical data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Test and train data split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.25, random_state=22)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObUWOKRzq-zd"
      },
      "source": [
        "# **Print shape of X_train, y_train,X_test and y_test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4_L88QnmxMI",
        "outputId": "91913058-c283-4545-be0c-248f2a22ddbe"
      },
      "source": [
        "print(\"X_train.shape: \", X_train.shape)\n",
        "print(\"Y_train.shape: \", Y_train.shape)\n",
        "print(\"X_test.shape: \", X_test.shape)\n",
        "print(\"Y_test.shape: \", Y_test.shape)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train.shape:  (426, 30)\n",
            "Y_train.shape:  (426,)\n",
            "X_test.shape:  (143, 30)\n",
            "Y_test.shape:  (143,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-5xK-n7nP6N"
      },
      "source": [
        "# **Apply existing model provided in the usecase**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgv1Rv6FmxJI",
        "outputId": "7629d791-9872-4e15-ad13-8e32e56c288f"
      },
      "source": [
        "model_1 = Sequential() # create a Sequential model\n",
        "\n",
        "model_1.add(Dense(32, input_dim=30, activation='relu')) # hidden layer\n",
        "model_1.add(Dense(1, activation='sigmoid')) # output layer (WHY 'sigmoid function!!!')\n",
        "\n",
        "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "# The returned history object holds a record of the loss values and metric values during training\n",
        "history =  model_1_fitted = model_1.fit(X_train, Y_train, epochs=200, verbose=1, shuffle=True)\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 4.0859 - acc: 0.6972\n",
            "Epoch 2/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.2697 - acc: 0.7700\n",
            "Epoch 3/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.7730 - acc: 0.7559\n",
            "Epoch 4/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.7771 - acc: 0.8005\n",
            "Epoch 5/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.5497 - acc: 0.8498\n",
            "Epoch 6/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 1.3450 - acc: 0.8521\n",
            "Epoch 7/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 1.2393 - acc: 0.8521\n",
            "Epoch 8/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 1.1319 - acc: 0.8568\n",
            "Epoch 9/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1504 - acc: 0.8521\n",
            "Epoch 10/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 1.0972 - acc: 0.8592\n",
            "Epoch 11/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.8446 - acc: 0.8498\n",
            "Epoch 12/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.9446 - acc: 0.8615\n",
            "Epoch 13/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.9223 - acc: 0.8333\n",
            "Epoch 14/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.9631 - acc: 0.8592\n",
            "Epoch 15/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.7508 - acc: 0.8779\n",
            "Epoch 16/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6667 - acc: 0.8779\n",
            "Epoch 17/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6018 - acc: 0.9038\n",
            "Epoch 18/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5723 - acc: 0.8826\n",
            "Epoch 19/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6972 - acc: 0.8545\n",
            "Epoch 20/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6134 - acc: 0.8944\n",
            "Epoch 21/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5709 - acc: 0.8850\n",
            "Epoch 22/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4794 - acc: 0.8967\n",
            "Epoch 23/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4606 - acc: 0.9014\n",
            "Epoch 24/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5051 - acc: 0.8897\n",
            "Epoch 25/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.4808 - acc: 0.8920\n",
            "Epoch 26/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6109 - acc: 0.8568\n",
            "Epoch 27/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4091 - acc: 0.9014\n",
            "Epoch 28/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.5675 - acc: 0.8826\n",
            "Epoch 29/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4162 - acc: 0.8967\n",
            "Epoch 30/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4142 - acc: 0.9014\n",
            "Epoch 31/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.4908 - acc: 0.8779\n",
            "Epoch 32/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4940 - acc: 0.8732\n",
            "Epoch 33/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5465 - acc: 0.8662\n",
            "Epoch 34/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5139 - acc: 0.8638\n",
            "Epoch 35/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.6518 - acc: 0.8474\n",
            "Epoch 36/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4087 - acc: 0.9014\n",
            "Epoch 37/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4240 - acc: 0.8967\n",
            "Epoch 38/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4526 - acc: 0.8967\n",
            "Epoch 39/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.4607 - acc: 0.8920\n",
            "Epoch 40/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3741 - acc: 0.8991\n",
            "Epoch 41/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.3175 - acc: 0.9155\n",
            "Epoch 42/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2922 - acc: 0.9178\n",
            "Epoch 43/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.3346 - acc: 0.9131\n",
            "Epoch 44/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2702 - acc: 0.9155\n",
            "Epoch 45/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2623 - acc: 0.9249\n",
            "Epoch 46/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3524 - acc: 0.9061\n",
            "Epoch 47/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4178 - acc: 0.8803\n",
            "Epoch 48/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3196 - acc: 0.9061\n",
            "Epoch 49/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2480 - acc: 0.9225\n",
            "Epoch 50/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3956 - acc: 0.8826\n",
            "Epoch 51/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3241 - acc: 0.9014\n",
            "Epoch 52/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2318 - acc: 0.9202\n",
            "Epoch 53/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2656 - acc: 0.9014\n",
            "Epoch 54/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3887 - acc: 0.8873\n",
            "Epoch 55/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3648 - acc: 0.9061\n",
            "Epoch 56/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2241 - acc: 0.9202\n",
            "Epoch 57/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2432 - acc: 0.9225\n",
            "Epoch 58/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.3218 - acc: 0.8897\n",
            "Epoch 59/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4482 - acc: 0.8850\n",
            "Epoch 60/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.2468 - acc: 0.9296\n",
            "Epoch 61/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2330 - acc: 0.9178\n",
            "Epoch 62/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2101 - acc: 0.9366\n",
            "Epoch 63/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3093 - acc: 0.9249\n",
            "Epoch 64/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2498 - acc: 0.9178\n",
            "Epoch 65/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2012 - acc: 0.9319\n",
            "Epoch 66/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1827 - acc: 0.9366\n",
            "Epoch 67/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1839 - acc: 0.9319\n",
            "Epoch 68/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2104 - acc: 0.9272\n",
            "Epoch 69/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2534 - acc: 0.9178\n",
            "Epoch 70/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3350 - acc: 0.8897\n",
            "Epoch 71/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3495 - acc: 0.9108\n",
            "Epoch 72/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.3289 - acc: 0.8991\n",
            "Epoch 73/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1913 - acc: 0.9296\n",
            "Epoch 74/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.2007 - acc: 0.9272\n",
            "Epoch 75/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2154 - acc: 0.9272\n",
            "Epoch 76/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1509 - acc: 0.9390\n",
            "Epoch 77/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1689 - acc: 0.9366\n",
            "Epoch 78/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1615 - acc: 0.9484\n",
            "Epoch 79/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1818 - acc: 0.9272\n",
            "Epoch 80/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1605 - acc: 0.9296\n",
            "Epoch 81/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1996 - acc: 0.9366\n",
            "Epoch 82/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1738 - acc: 0.9343\n",
            "Epoch 83/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1913 - acc: 0.9390\n",
            "Epoch 84/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1723 - acc: 0.9225\n",
            "Epoch 85/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1582 - acc: 0.9437\n",
            "Epoch 86/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1668 - acc: 0.9343\n",
            "Epoch 87/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1501 - acc: 0.9413\n",
            "Epoch 88/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1223 - acc: 0.9460\n",
            "Epoch 89/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1623 - acc: 0.9460\n",
            "Epoch 90/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.2121 - acc: 0.9296\n",
            "Epoch 91/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1725 - acc: 0.9202\n",
            "Epoch 92/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1572 - acc: 0.9343\n",
            "Epoch 93/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1832 - acc: 0.9296\n",
            "Epoch 94/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2408 - acc: 0.9085\n",
            "Epoch 95/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1948 - acc: 0.9225\n",
            "Epoch 96/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1389 - acc: 0.9437\n",
            "Epoch 97/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1267 - acc: 0.9507\n",
            "Epoch 98/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1399 - acc: 0.9413\n",
            "Epoch 99/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1707 - acc: 0.9296\n",
            "Epoch 100/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1650 - acc: 0.9296\n",
            "Epoch 101/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1419 - acc: 0.9484\n",
            "Epoch 102/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1596 - acc: 0.9460\n",
            "Epoch 103/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2055 - acc: 0.9296\n",
            "Epoch 104/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1765 - acc: 0.9390\n",
            "Epoch 105/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1294 - acc: 0.9390\n",
            "Epoch 106/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1430 - acc: 0.9366\n",
            "Epoch 107/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1271 - acc: 0.9413\n",
            "Epoch 108/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1805 - acc: 0.9413\n",
            "Epoch 109/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1898 - acc: 0.9319\n",
            "Epoch 110/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1621 - acc: 0.9319\n",
            "Epoch 111/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1390 - acc: 0.9413\n",
            "Epoch 112/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1324 - acc: 0.9390\n",
            "Epoch 113/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1709 - acc: 0.9249\n",
            "Epoch 114/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1669 - acc: 0.9413\n",
            "Epoch 115/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1438 - acc: 0.9366\n",
            "Epoch 116/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1217 - acc: 0.9484\n",
            "Epoch 117/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1859 - acc: 0.9319\n",
            "Epoch 118/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1369 - acc: 0.9577\n",
            "Epoch 119/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1598 - acc: 0.9272\n",
            "Epoch 120/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1866 - acc: 0.9249\n",
            "Epoch 121/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1531 - acc: 0.9460\n",
            "Epoch 122/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1751 - acc: 0.9390\n",
            "Epoch 123/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1643 - acc: 0.9319\n",
            "Epoch 124/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1216 - acc: 0.9484\n",
            "Epoch 125/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1275 - acc: 0.9507\n",
            "Epoch 126/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1170 - acc: 0.9531\n",
            "Epoch 127/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1091 - acc: 0.9554\n",
            "Epoch 128/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1114 - acc: 0.9554\n",
            "Epoch 129/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2304 - acc: 0.9108\n",
            "Epoch 130/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2269 - acc: 0.9155\n",
            "Epoch 131/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2216 - acc: 0.9249\n",
            "Epoch 132/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1213 - acc: 0.9484\n",
            "Epoch 133/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1294 - acc: 0.9437\n",
            "Epoch 134/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1242 - acc: 0.9484\n",
            "Epoch 135/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1960 - acc: 0.9249\n",
            "Epoch 136/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1422 - acc: 0.9343\n",
            "Epoch 137/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1109 - acc: 0.9460\n",
            "Epoch 138/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0991 - acc: 0.9507\n",
            "Epoch 139/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1315 - acc: 0.9484\n",
            "Epoch 140/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1286 - acc: 0.9507\n",
            "Epoch 141/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1420 - acc: 0.9460\n",
            "Epoch 142/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1156 - acc: 0.9531\n",
            "Epoch 143/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1196 - acc: 0.9390\n",
            "Epoch 144/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1800 - acc: 0.9366\n",
            "Epoch 145/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1281 - acc: 0.9460\n",
            "Epoch 146/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1213 - acc: 0.9484\n",
            "Epoch 147/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2694 - acc: 0.9108\n",
            "Epoch 148/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1732 - acc: 0.9366\n",
            "Epoch 149/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1205 - acc: 0.9413\n",
            "Epoch 150/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1496 - acc: 0.9390\n",
            "Epoch 151/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1460 - acc: 0.9343\n",
            "Epoch 152/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1684 - acc: 0.9366\n",
            "Epoch 153/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1933 - acc: 0.9296\n",
            "Epoch 154/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1374 - acc: 0.9484\n",
            "Epoch 155/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1418 - acc: 0.9484\n",
            "Epoch 156/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1509 - acc: 0.9390\n",
            "Epoch 157/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2247 - acc: 0.9225\n",
            "Epoch 158/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1068 - acc: 0.9531\n",
            "Epoch 159/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1094 - acc: 0.9507\n",
            "Epoch 160/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1005 - acc: 0.9577\n",
            "Epoch 161/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1103 - acc: 0.9554\n",
            "Epoch 162/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1234 - acc: 0.9531\n",
            "Epoch 163/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1007 - acc: 0.9577\n",
            "Epoch 164/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.1037 - acc: 0.9484\n",
            "Epoch 165/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1148 - acc: 0.9554\n",
            "Epoch 166/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0995 - acc: 0.9554\n",
            "Epoch 167/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1137 - acc: 0.9554\n",
            "Epoch 168/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1239 - acc: 0.9507\n",
            "Epoch 169/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1220 - acc: 0.9554\n",
            "Epoch 170/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1333 - acc: 0.9484\n",
            "Epoch 171/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0945 - acc: 0.9507\n",
            "Epoch 172/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1036 - acc: 0.9554\n",
            "Epoch 173/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0977 - acc: 0.9484\n",
            "Epoch 174/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0960 - acc: 0.9624\n",
            "Epoch 175/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1769 - acc: 0.9319\n",
            "Epoch 176/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1279 - acc: 0.9601\n",
            "Epoch 177/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1750 - acc: 0.9319\n",
            "Epoch 178/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1172 - acc: 0.9507\n",
            "Epoch 179/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0909 - acc: 0.9554\n",
            "Epoch 180/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1259 - acc: 0.9437\n",
            "Epoch 181/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1082 - acc: 0.9601\n",
            "Epoch 182/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.0917 - acc: 0.9531\n",
            "Epoch 183/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1011 - acc: 0.9507\n",
            "Epoch 184/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0862 - acc: 0.9577\n",
            "Epoch 185/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0978 - acc: 0.9577\n",
            "Epoch 186/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1328 - acc: 0.9531\n",
            "Epoch 187/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1632 - acc: 0.9319\n",
            "Epoch 188/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1031 - acc: 0.9554\n",
            "Epoch 189/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0923 - acc: 0.9648\n",
            "Epoch 190/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1072 - acc: 0.9577\n",
            "Epoch 191/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1390 - acc: 0.9319\n",
            "Epoch 192/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0953 - acc: 0.9601\n",
            "Epoch 193/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0898 - acc: 0.9554\n",
            "Epoch 194/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1165 - acc: 0.9484\n",
            "Epoch 195/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0995 - acc: 0.9577\n",
            "Epoch 196/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0910 - acc: 0.9624\n",
            "Epoch 197/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1446 - acc: 0.9507\n",
            "Epoch 198/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1406 - acc: 0.9437\n",
            "Epoch 199/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0914 - acc: 0.9624\n",
            "Epoch 200/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1566 - acc: 0.9390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PHqu3NgnWuI"
      },
      "source": [
        "# **Accuracy of the model 1**\n",
        "\n",
        "1. Prints the model summary\n",
        "2. evaluate loss and accuracy using test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "778DqyyfmxF2",
        "outputId": "5dfec58b-cfce-400f-8399-fb23ebd11496"
      },
      "source": [
        "print(model_1.summary())\n",
        "\n",
        "loss, accuracy = model_1.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(\"\\nModel 1 Accuracy: \", accuracy)\n",
        "print(\"Model 1 Loss: \", loss)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 32)                992       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,025\n",
            "Trainable params: 1,025\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "Model 1 Accuracy:  0.9370629191398621\n",
            "Model 1 Loss:  0.140098437666893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N54CQV9qnie9"
      },
      "source": [
        "# **Add more Dense layers to the existing code and check accuracy**\n",
        "\n",
        "\n",
        "\n",
        "*   Here I have added extra dense with 64 units and Relu as \n",
        "activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFd_xv_QmxB0"
      },
      "source": [
        "model_2 = Sequential() # create a Sequential model\n",
        "\n",
        "model_2.add(Dense(32, input_dim=30, activation='relu')) # hidden layer\n",
        "\n",
        "# Add more Dense layers to the existing code\n",
        "model_2.add(Dense(64, activation='relu')) \n",
        "\n",
        "model_2.add(Dense(1, activation='sigmoid')) # output layer \n",
        "\n",
        "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxD95rcgn0ki"
      },
      "source": [
        "# **Add the validation_data=(X_test, Y_test) attribute to .fit() method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clJsJV4Kmw6j",
        "outputId": "41a1afa4-32fa-4cc8-ff2c-d1cdae468860"
      },
      "source": [
        "# Add the validation_data=(X_test, Y_test) attribute to .fit() method\n",
        "history = model_2_fitted = model_2.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=200, verbose=1, shuffle=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "14/14 [==============================] - 1s 14ms/step - loss: 16.9642 - acc: 0.4249 - val_loss: 2.7844 - val_acc: 0.3497\n",
            "Epoch 2/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 3.7174 - acc: 0.4390 - val_loss: 2.3617 - val_acc: 0.5944\n",
            "Epoch 3/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 1.5606 - acc: 0.5869 - val_loss: 1.0797 - val_acc: 0.4336\n",
            "Epoch 4/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.7493 - acc: 0.7512 - val_loss: 0.5359 - val_acc: 0.7902\n",
            "Epoch 5/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.5501 - acc: 0.8286 - val_loss: 0.4857 - val_acc: 0.8881\n",
            "Epoch 6/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.4403 - acc: 0.8662 - val_loss: 0.3919 - val_acc: 0.8951\n",
            "Epoch 7/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4242 - acc: 0.8310 - val_loss: 0.4225 - val_acc: 0.7692\n",
            "Epoch 8/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.3703 - acc: 0.8615 - val_loss: 0.7399 - val_acc: 0.7692\n",
            "Epoch 9/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4141 - acc: 0.8568 - val_loss: 0.3960 - val_acc: 0.7902\n",
            "Epoch 10/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4165 - acc: 0.8451 - val_loss: 0.2919 - val_acc: 0.9161\n",
            "Epoch 11/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2882 - acc: 0.8873 - val_loss: 0.2673 - val_acc: 0.9161\n",
            "Epoch 12/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2867 - acc: 0.8850 - val_loss: 0.2573 - val_acc: 0.9091\n",
            "Epoch 13/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2642 - acc: 0.8920 - val_loss: 0.2815 - val_acc: 0.8951\n",
            "Epoch 14/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3204 - acc: 0.8732 - val_loss: 0.2711 - val_acc: 0.9091\n",
            "Epoch 15/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2690 - acc: 0.8967 - val_loss: 0.2441 - val_acc: 0.9161\n",
            "Epoch 16/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2787 - acc: 0.8991 - val_loss: 0.2457 - val_acc: 0.9091\n",
            "Epoch 17/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2687 - acc: 0.9038 - val_loss: 0.2616 - val_acc: 0.8881\n",
            "Epoch 18/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2454 - acc: 0.9085 - val_loss: 0.2355 - val_acc: 0.9021\n",
            "Epoch 19/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2479 - acc: 0.9014 - val_loss: 0.4239 - val_acc: 0.7972\n",
            "Epoch 20/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3294 - acc: 0.8732 - val_loss: 0.2815 - val_acc: 0.9091\n",
            "Epoch 21/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2540 - acc: 0.9038 - val_loss: 0.2662 - val_acc: 0.9091\n",
            "Epoch 22/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2633 - acc: 0.8991 - val_loss: 0.2753 - val_acc: 0.9021\n",
            "Epoch 23/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2397 - acc: 0.9038 - val_loss: 0.2900 - val_acc: 0.8881\n",
            "Epoch 24/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2336 - acc: 0.9038 - val_loss: 0.2369 - val_acc: 0.8951\n",
            "Epoch 25/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2681 - acc: 0.8944 - val_loss: 0.2399 - val_acc: 0.9021\n",
            "Epoch 26/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2416 - acc: 0.9061 - val_loss: 0.2217 - val_acc: 0.9021\n",
            "Epoch 27/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2398 - acc: 0.9038 - val_loss: 0.2302 - val_acc: 0.9091\n",
            "Epoch 28/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2177 - acc: 0.9155 - val_loss: 0.2051 - val_acc: 0.9161\n",
            "Epoch 29/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2993 - acc: 0.8873 - val_loss: 0.3817 - val_acc: 0.8671\n",
            "Epoch 30/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4693 - acc: 0.8310 - val_loss: 0.2965 - val_acc: 0.9021\n",
            "Epoch 31/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4042 - acc: 0.8756 - val_loss: 0.4925 - val_acc: 0.7902\n",
            "Epoch 32/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3697 - acc: 0.8709 - val_loss: 0.5759 - val_acc: 0.8462\n",
            "Epoch 33/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3224 - acc: 0.8826 - val_loss: 0.2589 - val_acc: 0.9091\n",
            "Epoch 34/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2362 - acc: 0.9225 - val_loss: 0.3075 - val_acc: 0.9021\n",
            "Epoch 35/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.4050 - acc: 0.8638 - val_loss: 0.4234 - val_acc: 0.8182\n",
            "Epoch 36/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2509 - acc: 0.9155 - val_loss: 0.2151 - val_acc: 0.8951\n",
            "Epoch 37/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2767 - acc: 0.9108 - val_loss: 0.3361 - val_acc: 0.9021\n",
            "Epoch 38/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2691 - acc: 0.8967 - val_loss: 0.2036 - val_acc: 0.8951\n",
            "Epoch 39/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2823 - acc: 0.8897 - val_loss: 0.1929 - val_acc: 0.9231\n",
            "Epoch 40/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.3063 - acc: 0.8850 - val_loss: 0.1963 - val_acc: 0.9161\n",
            "Epoch 41/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2780 - acc: 0.8991 - val_loss: 0.2377 - val_acc: 0.8951\n",
            "Epoch 42/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2139 - acc: 0.9319 - val_loss: 0.1954 - val_acc: 0.9091\n",
            "Epoch 43/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1994 - acc: 0.9390 - val_loss: 0.2076 - val_acc: 0.9021\n",
            "Epoch 44/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2139 - acc: 0.9202 - val_loss: 0.1838 - val_acc: 0.9161\n",
            "Epoch 45/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2267 - acc: 0.9108 - val_loss: 0.2889 - val_acc: 0.8811\n",
            "Epoch 46/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2219 - acc: 0.9249 - val_loss: 0.3974 - val_acc: 0.8741\n",
            "Epoch 47/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2650 - acc: 0.9014 - val_loss: 0.2698 - val_acc: 0.9021\n",
            "Epoch 48/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2298 - acc: 0.9131 - val_loss: 0.1847 - val_acc: 0.9301\n",
            "Epoch 49/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2755 - acc: 0.8897 - val_loss: 0.5977 - val_acc: 0.8392\n",
            "Epoch 50/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.3187 - acc: 0.8944 - val_loss: 0.2181 - val_acc: 0.8951\n",
            "Epoch 51/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2100 - acc: 0.9202 - val_loss: 0.2739 - val_acc: 0.9021\n",
            "Epoch 52/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1866 - acc: 0.9131 - val_loss: 0.1836 - val_acc: 0.9231\n",
            "Epoch 53/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1799 - acc: 0.9296 - val_loss: 0.1750 - val_acc: 0.9231\n",
            "Epoch 54/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2149 - acc: 0.9108 - val_loss: 0.2618 - val_acc: 0.9021\n",
            "Epoch 55/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2457 - acc: 0.9061 - val_loss: 0.1944 - val_acc: 0.9021\n",
            "Epoch 56/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2759 - acc: 0.8944 - val_loss: 0.5099 - val_acc: 0.8531\n",
            "Epoch 57/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2753 - acc: 0.8967 - val_loss: 0.1740 - val_acc: 0.9231\n",
            "Epoch 58/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1896 - acc: 0.9296 - val_loss: 0.1731 - val_acc: 0.9231\n",
            "Epoch 59/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1881 - acc: 0.9272 - val_loss: 0.1796 - val_acc: 0.9161\n",
            "Epoch 60/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2627 - acc: 0.8991 - val_loss: 0.3112 - val_acc: 0.8811\n",
            "Epoch 61/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2321 - acc: 0.8991 - val_loss: 0.3302 - val_acc: 0.8881\n",
            "Epoch 62/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2095 - acc: 0.9202 - val_loss: 0.2276 - val_acc: 0.9091\n",
            "Epoch 63/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2179 - acc: 0.9202 - val_loss: 0.1824 - val_acc: 0.9231\n",
            "Epoch 64/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.3700 - acc: 0.8732 - val_loss: 0.2128 - val_acc: 0.9021\n",
            "Epoch 65/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1931 - acc: 0.9249 - val_loss: 0.1740 - val_acc: 0.9161\n",
            "Epoch 66/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1869 - acc: 0.9319 - val_loss: 0.1760 - val_acc: 0.9161\n",
            "Epoch 67/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1722 - acc: 0.9366 - val_loss: 0.2417 - val_acc: 0.9021\n",
            "Epoch 68/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2120 - acc: 0.9249 - val_loss: 0.3012 - val_acc: 0.8881\n",
            "Epoch 69/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2706 - acc: 0.8967 - val_loss: 0.1697 - val_acc: 0.9301\n",
            "Epoch 70/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2352 - acc: 0.9155 - val_loss: 0.3075 - val_acc: 0.8741\n",
            "Epoch 71/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2029 - acc: 0.9249 - val_loss: 0.2030 - val_acc: 0.9231\n",
            "Epoch 72/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1955 - acc: 0.9366 - val_loss: 0.2317 - val_acc: 0.9161\n",
            "Epoch 73/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1856 - acc: 0.9225 - val_loss: 0.3622 - val_acc: 0.8671\n",
            "Epoch 74/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2413 - acc: 0.9131 - val_loss: 0.2731 - val_acc: 0.8951\n",
            "Epoch 75/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.3165 - acc: 0.8850 - val_loss: 0.3030 - val_acc: 0.8881\n",
            "Epoch 76/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.3369 - acc: 0.8944 - val_loss: 0.2229 - val_acc: 0.9021\n",
            "Epoch 77/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2510 - acc: 0.9178 - val_loss: 0.1801 - val_acc: 0.9161\n",
            "Epoch 78/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2099 - acc: 0.9272 - val_loss: 0.2048 - val_acc: 0.9231\n",
            "Epoch 79/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2042 - acc: 0.9272 - val_loss: 0.2009 - val_acc: 0.9091\n",
            "Epoch 80/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2433 - acc: 0.9038 - val_loss: 0.1673 - val_acc: 0.9231\n",
            "Epoch 81/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2247 - acc: 0.9155 - val_loss: 0.2237 - val_acc: 0.8951\n",
            "Epoch 82/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3164 - acc: 0.8850 - val_loss: 0.5542 - val_acc: 0.8531\n",
            "Epoch 83/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3016 - acc: 0.9038 - val_loss: 0.1632 - val_acc: 0.9301\n",
            "Epoch 84/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1590 - acc: 0.9366 - val_loss: 0.1536 - val_acc: 0.9441\n",
            "Epoch 85/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1790 - acc: 0.9319 - val_loss: 0.1807 - val_acc: 0.9161\n",
            "Epoch 86/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1572 - acc: 0.9249 - val_loss: 0.1597 - val_acc: 0.9371\n",
            "Epoch 87/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1475 - acc: 0.9484 - val_loss: 0.1608 - val_acc: 0.9301\n",
            "Epoch 88/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.3042 - acc: 0.8756 - val_loss: 0.6761 - val_acc: 0.8322\n",
            "Epoch 89/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4635 - acc: 0.8615 - val_loss: 0.1733 - val_acc: 0.9091\n",
            "Epoch 90/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1879 - acc: 0.9225 - val_loss: 0.1856 - val_acc: 0.9091\n",
            "Epoch 91/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1693 - acc: 0.9437 - val_loss: 0.2884 - val_acc: 0.8811\n",
            "Epoch 92/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2141 - acc: 0.9202 - val_loss: 0.2634 - val_acc: 0.9091\n",
            "Epoch 93/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2917 - acc: 0.9108 - val_loss: 0.7524 - val_acc: 0.7413\n",
            "Epoch 94/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2381 - acc: 0.9131 - val_loss: 0.1583 - val_acc: 0.9301\n",
            "Epoch 95/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1833 - acc: 0.9343 - val_loss: 0.2256 - val_acc: 0.9231\n",
            "Epoch 96/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1604 - acc: 0.9390 - val_loss: 0.2305 - val_acc: 0.9231\n",
            "Epoch 97/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1857 - acc: 0.9366 - val_loss: 0.1997 - val_acc: 0.9161\n",
            "Epoch 98/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1793 - acc: 0.9249 - val_loss: 0.1531 - val_acc: 0.9301\n",
            "Epoch 99/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1902 - acc: 0.9296 - val_loss: 0.1760 - val_acc: 0.9161\n",
            "Epoch 100/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1836 - acc: 0.9296 - val_loss: 0.1567 - val_acc: 0.9371\n",
            "Epoch 101/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1426 - acc: 0.9437 - val_loss: 0.2224 - val_acc: 0.9021\n",
            "Epoch 102/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2001 - acc: 0.9296 - val_loss: 0.6859 - val_acc: 0.8392\n",
            "Epoch 103/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2689 - acc: 0.9038 - val_loss: 0.6243 - val_acc: 0.7832\n",
            "Epoch 104/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.5298 - acc: 0.8286 - val_loss: 0.1602 - val_acc: 0.9231\n",
            "Epoch 105/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3587 - acc: 0.8920 - val_loss: 0.1614 - val_acc: 0.9301\n",
            "Epoch 106/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.3403 - acc: 0.8756 - val_loss: 0.1620 - val_acc: 0.9231\n",
            "Epoch 107/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.3016 - acc: 0.9014 - val_loss: 0.2189 - val_acc: 0.9231\n",
            "Epoch 108/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1870 - acc: 0.9319 - val_loss: 0.1529 - val_acc: 0.9301\n",
            "Epoch 109/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1430 - acc: 0.9484 - val_loss: 0.1467 - val_acc: 0.9371\n",
            "Epoch 110/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1875 - acc: 0.9272 - val_loss: 0.1598 - val_acc: 0.9231\n",
            "Epoch 111/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1548 - acc: 0.9390 - val_loss: 0.1465 - val_acc: 0.9441\n",
            "Epoch 112/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1370 - acc: 0.9413 - val_loss: 0.2401 - val_acc: 0.8881\n",
            "Epoch 113/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2498 - acc: 0.9155 - val_loss: 0.1472 - val_acc: 0.9371\n",
            "Epoch 114/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1470 - acc: 0.9390 - val_loss: 0.1736 - val_acc: 0.9161\n",
            "Epoch 115/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2144 - acc: 0.9178 - val_loss: 0.2652 - val_acc: 0.9161\n",
            "Epoch 116/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1551 - acc: 0.9343 - val_loss: 0.1629 - val_acc: 0.9231\n",
            "Epoch 117/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1330 - acc: 0.9437 - val_loss: 0.1994 - val_acc: 0.9301\n",
            "Epoch 118/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1452 - acc: 0.9413 - val_loss: 0.1782 - val_acc: 0.9371\n",
            "Epoch 119/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1411 - acc: 0.9390 - val_loss: 0.1520 - val_acc: 0.9371\n",
            "Epoch 120/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1485 - acc: 0.9413 - val_loss: 0.1586 - val_acc: 0.9301\n",
            "Epoch 121/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1243 - acc: 0.9484 - val_loss: 0.1412 - val_acc: 0.9441\n",
            "Epoch 122/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1434 - acc: 0.9343 - val_loss: 0.1656 - val_acc: 0.9231\n",
            "Epoch 123/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2057 - acc: 0.9296 - val_loss: 0.2100 - val_acc: 0.9231\n",
            "Epoch 124/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1653 - acc: 0.9437 - val_loss: 0.2070 - val_acc: 0.9301\n",
            "Epoch 125/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2145 - acc: 0.9249 - val_loss: 0.1840 - val_acc: 0.9301\n",
            "Epoch 126/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2144 - acc: 0.9343 - val_loss: 0.4718 - val_acc: 0.8392\n",
            "Epoch 127/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.3487 - acc: 0.8709 - val_loss: 0.3596 - val_acc: 0.8951\n",
            "Epoch 128/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1543 - acc: 0.9484 - val_loss: 0.1579 - val_acc: 0.9231\n",
            "Epoch 129/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1323 - acc: 0.9554 - val_loss: 0.2058 - val_acc: 0.9301\n",
            "Epoch 130/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1738 - acc: 0.9202 - val_loss: 0.1857 - val_acc: 0.9161\n",
            "Epoch 131/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1699 - acc: 0.9390 - val_loss: 0.1425 - val_acc: 0.9441\n",
            "Epoch 132/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1330 - acc: 0.9366 - val_loss: 0.1584 - val_acc: 0.9231\n",
            "Epoch 133/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1205 - acc: 0.9507 - val_loss: 0.1414 - val_acc: 0.9441\n",
            "Epoch 134/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1238 - acc: 0.9413 - val_loss: 0.1398 - val_acc: 0.9441\n",
            "Epoch 135/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1431 - acc: 0.9460 - val_loss: 0.1798 - val_acc: 0.9231\n",
            "Epoch 136/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1283 - acc: 0.9484 - val_loss: 0.2521 - val_acc: 0.9091\n",
            "Epoch 137/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1625 - acc: 0.9343 - val_loss: 0.1996 - val_acc: 0.9161\n",
            "Epoch 138/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1869 - acc: 0.9296 - val_loss: 0.1418 - val_acc: 0.9441\n",
            "Epoch 139/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1406 - acc: 0.9413 - val_loss: 0.1814 - val_acc: 0.9371\n",
            "Epoch 140/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1496 - acc: 0.9413 - val_loss: 0.5899 - val_acc: 0.7832\n",
            "Epoch 141/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2879 - acc: 0.8920 - val_loss: 0.2412 - val_acc: 0.9231\n",
            "Epoch 142/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1373 - acc: 0.9413 - val_loss: 0.1712 - val_acc: 0.9301\n",
            "Epoch 143/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1714 - acc: 0.9343 - val_loss: 0.4145 - val_acc: 0.8811\n",
            "Epoch 144/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2716 - acc: 0.9178 - val_loss: 0.2928 - val_acc: 0.8811\n",
            "Epoch 145/200\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2238 - acc: 0.9131 - val_loss: 0.2993 - val_acc: 0.9091\n",
            "Epoch 146/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1982 - acc: 0.9319 - val_loss: 0.1920 - val_acc: 0.9231\n",
            "Epoch 147/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2893 - acc: 0.8920 - val_loss: 0.7126 - val_acc: 0.8601\n",
            "Epoch 148/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.3301 - acc: 0.9061 - val_loss: 0.1941 - val_acc: 0.9231\n",
            "Epoch 149/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1907 - acc: 0.9131 - val_loss: 0.3084 - val_acc: 0.8951\n",
            "Epoch 150/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1725 - acc: 0.9272 - val_loss: 0.1469 - val_acc: 0.9441\n",
            "Epoch 151/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1472 - acc: 0.9460 - val_loss: 0.3291 - val_acc: 0.8951\n",
            "Epoch 152/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1692 - acc: 0.9343 - val_loss: 0.1598 - val_acc: 0.9231\n",
            "Epoch 153/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1297 - acc: 0.9437 - val_loss: 0.1595 - val_acc: 0.9371\n",
            "Epoch 154/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1535 - acc: 0.9343 - val_loss: 0.2157 - val_acc: 0.9091\n",
            "Epoch 155/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2025 - acc: 0.9272 - val_loss: 0.2028 - val_acc: 0.9301\n",
            "Epoch 156/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1459 - acc: 0.9343 - val_loss: 0.1874 - val_acc: 0.9301\n",
            "Epoch 157/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2121 - acc: 0.9225 - val_loss: 0.1433 - val_acc: 0.9441\n",
            "Epoch 158/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2216 - acc: 0.9296 - val_loss: 0.1642 - val_acc: 0.9301\n",
            "Epoch 159/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2495 - acc: 0.9131 - val_loss: 0.2408 - val_acc: 0.9091\n",
            "Epoch 160/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4010 - acc: 0.8592 - val_loss: 1.0277 - val_acc: 0.8112\n",
            "Epoch 161/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5117 - acc: 0.8615 - val_loss: 0.5451 - val_acc: 0.8322\n",
            "Epoch 162/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.3245 - acc: 0.9038 - val_loss: 0.2099 - val_acc: 0.9231\n",
            "Epoch 163/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1609 - acc: 0.9296 - val_loss: 0.5017 - val_acc: 0.8881\n",
            "Epoch 164/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2457 - acc: 0.9225 - val_loss: 0.1546 - val_acc: 0.9441\n",
            "Epoch 165/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2167 - acc: 0.9202 - val_loss: 0.1658 - val_acc: 0.9441\n",
            "Epoch 166/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1933 - acc: 0.9225 - val_loss: 0.1539 - val_acc: 0.9371\n",
            "Epoch 167/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1232 - acc: 0.9507 - val_loss: 0.1888 - val_acc: 0.9301\n",
            "Epoch 168/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1150 - acc: 0.9507 - val_loss: 0.1504 - val_acc: 0.9371\n",
            "Epoch 169/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1136 - acc: 0.9531 - val_loss: 0.1746 - val_acc: 0.9371\n",
            "Epoch 170/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1294 - acc: 0.9531 - val_loss: 0.1934 - val_acc: 0.9371\n",
            "Epoch 171/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1562 - acc: 0.9366 - val_loss: 0.5314 - val_acc: 0.8252\n",
            "Epoch 172/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2739 - acc: 0.9061 - val_loss: 0.1743 - val_acc: 0.9231\n",
            "Epoch 173/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1669 - acc: 0.9390 - val_loss: 0.1643 - val_acc: 0.9301\n",
            "Epoch 174/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2209 - acc: 0.9272 - val_loss: 0.1572 - val_acc: 0.9301\n",
            "Epoch 175/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1309 - acc: 0.9460 - val_loss: 0.2089 - val_acc: 0.9301\n",
            "Epoch 176/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.2332 - acc: 0.9225 - val_loss: 0.1703 - val_acc: 0.9161\n",
            "Epoch 177/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1232 - acc: 0.9577 - val_loss: 0.2475 - val_acc: 0.9301\n",
            "Epoch 178/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1411 - acc: 0.9437 - val_loss: 0.1572 - val_acc: 0.9371\n",
            "Epoch 179/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1151 - acc: 0.9531 - val_loss: 0.1417 - val_acc: 0.9441\n",
            "Epoch 180/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1205 - acc: 0.9390 - val_loss: 0.1390 - val_acc: 0.9371\n",
            "Epoch 181/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1166 - acc: 0.9390 - val_loss: 0.1577 - val_acc: 0.9371\n",
            "Epoch 182/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1265 - acc: 0.9507 - val_loss: 0.1606 - val_acc: 0.9231\n",
            "Epoch 183/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1339 - acc: 0.9366 - val_loss: 0.1752 - val_acc: 0.9161\n",
            "Epoch 184/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1158 - acc: 0.9554 - val_loss: 0.1741 - val_acc: 0.9371\n",
            "Epoch 185/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1323 - acc: 0.9413 - val_loss: 0.1548 - val_acc: 0.9301\n",
            "Epoch 186/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1000 - acc: 0.9554 - val_loss: 0.1878 - val_acc: 0.9161\n",
            "Epoch 187/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1160 - acc: 0.9531 - val_loss: 0.3605 - val_acc: 0.8951\n",
            "Epoch 188/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1960 - acc: 0.9343 - val_loss: 0.2692 - val_acc: 0.9021\n",
            "Epoch 189/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1378 - acc: 0.9413 - val_loss: 0.1359 - val_acc: 0.9510\n",
            "Epoch 190/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0998 - acc: 0.9577 - val_loss: 0.1915 - val_acc: 0.9161\n",
            "Epoch 191/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1265 - acc: 0.9390 - val_loss: 0.1438 - val_acc: 0.9441\n",
            "Epoch 192/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1480 - acc: 0.9437 - val_loss: 0.1508 - val_acc: 0.9441\n",
            "Epoch 193/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.0996 - acc: 0.9531 - val_loss: 0.2476 - val_acc: 0.9231\n",
            "Epoch 194/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1465 - acc: 0.9413 - val_loss: 0.1383 - val_acc: 0.9441\n",
            "Epoch 195/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1146 - acc: 0.9507 - val_loss: 0.1369 - val_acc: 0.9371\n",
            "Epoch 196/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1209 - acc: 0.9507 - val_loss: 0.1586 - val_acc: 0.9161\n",
            "Epoch 197/200\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1724 - acc: 0.9319 - val_loss: 0.1395 - val_acc: 0.9580\n",
            "Epoch 198/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1286 - acc: 0.9484 - val_loss: 0.1350 - val_acc: 0.9371\n",
            "Epoch 199/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1369 - acc: 0.9484 - val_loss: 0.1339 - val_acc: 0.9510\n",
            "Epoch 200/200\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1124 - acc: 0.9554 - val_loss: 0.1714 - val_acc: 0.9161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wHOwMCqoAJu"
      },
      "source": [
        "# **Accuracy of the dense model 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sCaCLrkmw2B",
        "outputId": "95d27e06-7fe5-4d7f-b1c7-7d4880f229b4"
      },
      "source": [
        "print(model_2.summary())\n",
        "\n",
        "loss, accuracy = model_2.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(\"\\nModel 2 Accuracy with added dense layers: \", accuracy)\n",
        "print(\"Model 2 Loss with added dense layers: \", loss)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 32)                992       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 3,169\n",
            "Trainable params: 3,169\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "Model 2 Accuracy with added dense layers:  0.9160839319229126\n",
            "Model 2 Loss with added dense layers:  0.17141962051391602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWnhl52Lo4MH"
      },
      "source": [
        "# **Plot the accuracy for training and validation in the same plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "jtFQBdZmmwwL",
        "outputId": "17bf491d-27d0-4182-e40e-a477d1914702"
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Plot of Model Accuracy on Train and Validation Datasets')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdgmVe44o9sL"
      },
      "source": [
        "# **Normalize the data before feeding the data to the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yFxW87_mwo-"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "X_scaled = sc.fit_transform(X)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=22)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULt6TseqpHw5",
        "outputId": "2543cf18-5a0d-43e4-e476-7538dc4be261"
      },
      "source": [
        "# The returned history object holds a record of the loss values and metric values during training\n",
        "model_2_fitted = model_2.fit(X_train, Y_train, epochs=200, verbose=1, shuffle=True)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4374 - acc: 0.8451\n",
            "Epoch 2/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2257 - acc: 0.9061\n",
            "Epoch 3/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.1166 - acc: 0.9554\n",
            "Epoch 4/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0717 - acc: 0.9718\n",
            "Epoch 5/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0514 - acc: 0.9859\n",
            "Epoch 6/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0418 - acc: 0.9883\n",
            "Epoch 7/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0386 - acc: 0.9930\n",
            "Epoch 8/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0308 - acc: 0.9953\n",
            "Epoch 9/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0294 - acc: 0.9930\n",
            "Epoch 10/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0244 - acc: 0.9977\n",
            "Epoch 11/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0224 - acc: 0.9977\n",
            "Epoch 12/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0199 - acc: 0.9977\n",
            "Epoch 13/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0181 - acc: 0.9977\n",
            "Epoch 14/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0168 - acc: 0.9977\n",
            "Epoch 15/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0160 - acc: 0.9977\n",
            "Epoch 16/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0149 - acc: 0.9977\n",
            "Epoch 17/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0142 - acc: 0.9977\n",
            "Epoch 18/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0127 - acc: 0.9977\n",
            "Epoch 19/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0117 - acc: 0.9977\n",
            "Epoch 20/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0113 - acc: 0.9977\n",
            "Epoch 21/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.0109 - acc: 0.9977\n",
            "Epoch 22/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.0102 - acc: 0.9977\n",
            "Epoch 23/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0103 - acc: 0.9977\n",
            "Epoch 24/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0095 - acc: 0.9977\n",
            "Epoch 25/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0093 - acc: 0.9977\n",
            "Epoch 26/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0084 - acc: 0.9977\n",
            "Epoch 27/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0081 - acc: 0.9977\n",
            "Epoch 28/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0078 - acc: 0.9977\n",
            "Epoch 29/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0073 - acc: 0.9977\n",
            "Epoch 30/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0073 - acc: 0.9977\n",
            "Epoch 31/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0069 - acc: 0.9977\n",
            "Epoch 32/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0063 - acc: 0.9977\n",
            "Epoch 33/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0062 - acc: 0.9977\n",
            "Epoch 34/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0061 - acc: 0.9977\n",
            "Epoch 35/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0057 - acc: 0.9977\n",
            "Epoch 36/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0055 - acc: 0.9977\n",
            "Epoch 37/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0052 - acc: 0.9977\n",
            "Epoch 38/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0050 - acc: 0.9977\n",
            "Epoch 39/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0050 - acc: 0.9977\n",
            "Epoch 40/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0046 - acc: 0.9977\n",
            "Epoch 41/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0046 - acc: 0.9977\n",
            "Epoch 42/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0041 - acc: 0.9977\n",
            "Epoch 43/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0050 - acc: 0.9977\n",
            "Epoch 44/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0040 - acc: 0.9977\n",
            "Epoch 45/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0037 - acc: 0.9977\n",
            "Epoch 46/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0037 - acc: 0.9977\n",
            "Epoch 47/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0035 - acc: 0.9977\n",
            "Epoch 48/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0032 - acc: 1.0000\n",
            "Epoch 49/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 50/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0030 - acc: 1.0000\n",
            "Epoch 51/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 52/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 53/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 54/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 55/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 56/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 57/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 58/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 59/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 60/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 61/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 62/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 63/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 64/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 65/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 66/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 67/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 68/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 69/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 70/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 71/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 72/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 73/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 9.9123e-04 - acc: 1.0000\n",
            "Epoch 74/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 9.6686e-04 - acc: 1.0000\n",
            "Epoch 75/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 9.2184e-04 - acc: 1.0000\n",
            "Epoch 76/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 8.7491e-04 - acc: 1.0000\n",
            "Epoch 77/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 8.3425e-04 - acc: 1.0000\n",
            "Epoch 78/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 8.4319e-04 - acc: 1.0000\n",
            "Epoch 79/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 8.0881e-04 - acc: 1.0000\n",
            "Epoch 80/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 7.4586e-04 - acc: 1.0000\n",
            "Epoch 81/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 7.3577e-04 - acc: 1.0000\n",
            "Epoch 82/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 6.9943e-04 - acc: 1.0000\n",
            "Epoch 83/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 6.4239e-04 - acc: 1.0000\n",
            "Epoch 84/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 6.0279e-04 - acc: 1.0000\n",
            "Epoch 85/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 5.8224e-04 - acc: 1.0000\n",
            "Epoch 86/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 5.6021e-04 - acc: 1.0000\n",
            "Epoch 87/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 5.4372e-04 - acc: 1.0000\n",
            "Epoch 88/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 5.2572e-04 - acc: 1.0000\n",
            "Epoch 89/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 5.0694e-04 - acc: 1.0000\n",
            "Epoch 90/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 4.8366e-04 - acc: 1.0000\n",
            "Epoch 91/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 4.7211e-04 - acc: 1.0000\n",
            "Epoch 92/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 4.5994e-04 - acc: 1.0000\n",
            "Epoch 93/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 4.6034e-04 - acc: 1.0000\n",
            "Epoch 94/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 4.2694e-04 - acc: 1.0000\n",
            "Epoch 95/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 4.2491e-04 - acc: 1.0000\n",
            "Epoch 96/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 4.0945e-04 - acc: 1.0000\n",
            "Epoch 97/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 3.8719e-04 - acc: 1.0000\n",
            "Epoch 98/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 3.8575e-04 - acc: 1.0000\n",
            "Epoch 99/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 3.6363e-04 - acc: 1.0000\n",
            "Epoch 100/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 3.6441e-04 - acc: 1.0000\n",
            "Epoch 101/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 3.4524e-04 - acc: 1.0000\n",
            "Epoch 102/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 3.4577e-04 - acc: 1.0000\n",
            "Epoch 103/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 3.2900e-04 - acc: 1.0000\n",
            "Epoch 104/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 3.2274e-04 - acc: 1.0000\n",
            "Epoch 105/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 3.1341e-04 - acc: 1.0000\n",
            "Epoch 106/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 3.0495e-04 - acc: 1.0000\n",
            "Epoch 107/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.9517e-04 - acc: 1.0000\n",
            "Epoch 108/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.8805e-04 - acc: 1.0000\n",
            "Epoch 109/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.8028e-04 - acc: 1.0000\n",
            "Epoch 110/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.7954e-04 - acc: 1.0000\n",
            "Epoch 111/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.7378e-04 - acc: 1.0000\n",
            "Epoch 112/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.6398e-04 - acc: 1.0000\n",
            "Epoch 113/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.5732e-04 - acc: 1.0000\n",
            "Epoch 114/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.4984e-04 - acc: 1.0000\n",
            "Epoch 115/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.3961e-04 - acc: 1.0000\n",
            "Epoch 116/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.3843e-04 - acc: 1.0000\n",
            "Epoch 117/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.2967e-04 - acc: 1.0000\n",
            "Epoch 118/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.2650e-04 - acc: 1.0000\n",
            "Epoch 119/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.2289e-04 - acc: 1.0000\n",
            "Epoch 120/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.1472e-04 - acc: 1.0000\n",
            "Epoch 121/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.1062e-04 - acc: 1.0000\n",
            "Epoch 122/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.0972e-04 - acc: 1.0000\n",
            "Epoch 123/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 2.0766e-04 - acc: 1.0000\n",
            "Epoch 124/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.9944e-04 - acc: 1.0000\n",
            "Epoch 125/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.9513e-04 - acc: 1.0000\n",
            "Epoch 126/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.8951e-04 - acc: 1.0000\n",
            "Epoch 127/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.8614e-04 - acc: 1.0000\n",
            "Epoch 128/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.8581e-04 - acc: 1.0000\n",
            "Epoch 129/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.8060e-04 - acc: 1.0000\n",
            "Epoch 130/200\n",
            "14/14 [==============================] - 0s 1ms/step - loss: 1.7512e-04 - acc: 1.0000\n",
            "Epoch 131/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.7007e-04 - acc: 1.0000\n",
            "Epoch 132/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.6824e-04 - acc: 1.0000\n",
            "Epoch 133/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.6527e-04 - acc: 1.0000\n",
            "Epoch 134/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.6443e-04 - acc: 1.0000\n",
            "Epoch 135/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.6288e-04 - acc: 1.0000\n",
            "Epoch 136/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.5457e-04 - acc: 1.0000\n",
            "Epoch 137/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 1.5499e-04 - acc: 1.0000\n",
            "Epoch 138/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.4869e-04 - acc: 1.0000\n",
            "Epoch 139/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.4766e-04 - acc: 1.0000\n",
            "Epoch 140/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.4553e-04 - acc: 1.0000\n",
            "Epoch 141/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.4171e-04 - acc: 1.0000\n",
            "Epoch 142/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.4027e-04 - acc: 1.0000\n",
            "Epoch 143/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.3730e-04 - acc: 1.0000\n",
            "Epoch 144/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.3284e-04 - acc: 1.0000\n",
            "Epoch 145/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.3139e-04 - acc: 1.0000\n",
            "Epoch 146/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.2891e-04 - acc: 1.0000\n",
            "Epoch 147/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.2677e-04 - acc: 1.0000\n",
            "Epoch 148/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.2358e-04 - acc: 1.0000\n",
            "Epoch 149/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.2096e-04 - acc: 1.0000\n",
            "Epoch 150/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1932e-04 - acc: 1.0000\n",
            "Epoch 151/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1681e-04 - acc: 1.0000\n",
            "Epoch 152/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1488e-04 - acc: 1.0000\n",
            "Epoch 153/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1418e-04 - acc: 1.0000\n",
            "Epoch 154/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1067e-04 - acc: 1.0000\n",
            "Epoch 155/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0842e-04 - acc: 1.0000\n",
            "Epoch 156/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0689e-04 - acc: 1.0000\n",
            "Epoch 157/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0894e-04 - acc: 1.0000\n",
            "Epoch 158/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.1078e-04 - acc: 1.0000\n",
            "Epoch 159/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0512e-04 - acc: 1.0000\n",
            "Epoch 160/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0152e-04 - acc: 1.0000\n",
            "Epoch 161/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 1.0179e-04 - acc: 1.0000\n",
            "Epoch 162/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 9.8033e-05 - acc: 1.0000\n",
            "Epoch 163/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 9.5415e-05 - acc: 1.0000\n",
            "Epoch 164/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 9.3699e-05 - acc: 1.0000\n",
            "Epoch 165/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 9.2060e-05 - acc: 1.0000\n",
            "Epoch 166/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 9.1056e-05 - acc: 1.0000\n",
            "Epoch 167/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 8.9488e-05 - acc: 1.0000\n",
            "Epoch 168/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 8.7923e-05 - acc: 1.0000\n",
            "Epoch 169/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 8.6872e-05 - acc: 1.0000\n",
            "Epoch 170/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 8.5121e-05 - acc: 1.0000\n",
            "Epoch 171/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 8.3230e-05 - acc: 1.0000\n",
            "Epoch 172/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 8.3313e-05 - acc: 1.0000\n",
            "Epoch 173/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 8.1510e-05 - acc: 1.0000\n",
            "Epoch 174/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 8.0083e-05 - acc: 1.0000\n",
            "Epoch 175/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 7.8881e-05 - acc: 1.0000\n",
            "Epoch 176/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 7.7724e-05 - acc: 1.0000\n",
            "Epoch 177/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 7.6654e-05 - acc: 1.0000\n",
            "Epoch 178/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 7.5653e-05 - acc: 1.0000\n",
            "Epoch 179/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 7.4076e-05 - acc: 1.0000\n",
            "Epoch 180/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 7.3126e-05 - acc: 1.0000\n",
            "Epoch 181/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 7.1922e-05 - acc: 1.0000\n",
            "Epoch 182/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 7.1188e-05 - acc: 1.0000\n",
            "Epoch 183/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 7.0572e-05 - acc: 1.0000\n",
            "Epoch 184/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 6.8830e-05 - acc: 1.0000\n",
            "Epoch 185/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 6.8263e-05 - acc: 1.0000\n",
            "Epoch 186/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 6.6837e-05 - acc: 1.0000\n",
            "Epoch 187/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 6.5447e-05 - acc: 1.0000\n",
            "Epoch 188/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 6.5144e-05 - acc: 1.0000\n",
            "Epoch 189/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 6.4434e-05 - acc: 1.0000\n",
            "Epoch 190/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 6.3057e-05 - acc: 1.0000\n",
            "Epoch 191/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 6.2040e-05 - acc: 1.0000\n",
            "Epoch 192/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 6.1008e-05 - acc: 1.0000\n",
            "Epoch 193/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 6.0391e-05 - acc: 1.0000\n",
            "Epoch 194/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 5.8619e-05 - acc: 1.0000\n",
            "Epoch 195/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 6.0432e-05 - acc: 1.0000\n",
            "Epoch 196/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 5.8543e-05 - acc: 1.0000\n",
            "Epoch 197/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 5.6783e-05 - acc: 1.0000\n",
            "Epoch 198/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 5.7240e-05 - acc: 1.0000\n",
            "Epoch 199/200\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 5.4897e-05 - acc: 1.0000\n",
            "Epoch 200/200\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 5.3795e-05 - acc: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC-V44dopOTM"
      },
      "source": [
        "# **Accuracy of the model 2 with normalized data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7Vbn-D_pHty",
        "outputId": "fe943b11-97fa-4aca-aaa9-ef3af87320f7"
      },
      "source": [
        "loss, accuracy = model_2.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(\"\\nModel_2 Accuracy after data normalization: \", accuracy)\n",
        "print(\"Model_2 Loss after data normalization: \", loss)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model_2 Accuracy after data normalization:  0.9510489702224731\n",
            "Model_2 Loss after data normalization:  0.6252762675285339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCVxDV4F2xOM"
      },
      "source": [
        "# **Summary**\n",
        "\n",
        "IN the part 2 we used Breast Cancer dataset. First, we trained model using 32 units of hidden dense layer neurons  and trained data, tested data and got accuracy of: \n",
        "\n",
        "**Model 1 Accuracy:  0.9370629191398621**\n",
        "\n",
        "**Model 1 Loss:  0.140098437666893**\n",
        "\n",
        "Second, Added more dense layer of 64 units, trained data. Tested data by passing vaidatation X_test and y-test and got accuracy of :\n",
        "\n",
        "**Model 2 Accuracy with added dense layers:  0.9160839319229126**\n",
        "\n",
        "**Model 2 Loss with added dense layers:  0.17141962051391602**\n",
        "\n",
        "Accuracy got decreased after adding extra dense layer. Loss percentage increased.\n",
        "\n",
        "Third, we normalized data using standardscalar() and tested on second model architecture and predicted test accuracy :\n",
        "\n",
        "**Model_2 Accuracy after data normalization:  0.9510489702224731**\n",
        "\n",
        "**Model_2 Loss after data normalization:  0.6252762675285339**\n",
        "\n",
        "After normalizing data accuracy agin got increased for model_2 (in this model we added extra layer of dence)\n"
      ]
    }
  ]
}